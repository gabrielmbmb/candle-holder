use candle_holder::{Error, Result};
use candle_holder_tokenizers::Tokenizer;
use std::io::{self, Write};

/// A trait for streamers that receives tokens generated by `generate` method using an
/// auto-regressive model. Streamers can be used to handle tokens as they are generated by the
/// model.
pub trait TokenStreamer<'a> {
    /// Receives a sequence of tokens generated by the model.
    fn put(&mut self, tokens: &[Vec<u32>]) -> Result<()>;
    /// Called when the generation is finished
    fn end(&mut self) -> Result<()>;
}

/// A streamer that prints the generated tokens to the console.
pub struct TextStreamer<'a> {
    /// The tokenizer used to decode the tokens into text.
    tokenizer: &'a Box<dyn Tokenizer>,
    /// Whether to skip the prompt when decoding the tokens into text.
    skip_prompt: bool,
    /// Whether to skip special tokens when decoding the tokens
    skip_special_tokens: bool,
    /// Whether the next tokens are part of the prompt.
    next_tokens_are_prompt: bool,
    /// A cache to store the tokens until a printable text is found.
    token_cache: Vec<u32>,
    /// The length of text that can be printed from the token cache.
    print_len: usize,
}

impl<'a> TextStreamer<'a> {
    /// Creates a new `TextStreamer` with the given tokenizer and whether to skip special
    /// tokens when decoding the tokens.
    ///
    /// # Arguments
    ///
    /// * `tokenizer` - The tokenizer used to decode the tokens into text.
    /// * `skip_prompt` - Whether to skip the prompt when decoding the tokens into text.
    /// * `skip_special_tokens` - Whether to skip special tokens when decoding the tokens.
    ///
    /// # Returns
    ///
    /// A new `TextStreamer`.
    pub fn new(
        tokenizer: &'a Box<dyn Tokenizer>,
        skip_prompt: bool,
        skip_special_tokens: bool,
    ) -> Self {
        TextStreamer {
            tokenizer,
            skip_prompt,
            skip_special_tokens,
            next_tokens_are_prompt: true,
            token_cache: vec![],
            print_len: 0,
        }
    }
}

impl<'a> TextStreamer<'a> {
    fn process_text(&mut self, full_text: &str) -> Option<String> {
        let new_text = &full_text[self.print_len..];

        if new_text.ends_with('\n') {
            self.print_len = 0;
            self.token_cache.clear();
            return Some(new_text.to_string());
        }

        if let Some(last_char) = new_text.chars().last() {
            if is_chinese_char(last_char) {
                self.print_len += new_text.len();
                return Some(new_text.to_string());
            }
        }

        // If we find a space, then we assume we have a word that can be printed
        new_text.rfind(' ').map(|last_space| {
            let printable = &new_text[..=last_space];
            self.print_len += printable.len();
            printable.to_string()
        })
    }

    fn print(&self, text: String, stream_end: bool) -> Result<()> {
        print!("{}{}", text, if stream_end { "\n" } else { "" });
        io::stdout().flush().map_err(Error::wrap)
    }
}

impl<'a> TokenStreamer<'a> for TextStreamer<'a> {
    fn put(&mut self, tokens: &[Vec<u32>]) -> Result<()> {
        if tokens.len() > 1 {
            return Err(Error::msg(
                "`TextStreamer` can only handle one sequence of tokens at a time.",
            ));
        }

        if self.skip_prompt && self.next_tokens_are_prompt {
            self.next_tokens_are_prompt = false;
            return Ok(());
        }

        self.token_cache.extend_from_slice(&tokens[0]);
        let text = self
            .tokenizer
            .decode(&self.token_cache, self.skip_special_tokens)?;

        if let Some(printable_text) = self.process_text(&text) {
            self.print(printable_text, false)?;
        }

        Ok(())
    }

    fn end(&mut self) -> Result<()> {
        if !self.token_cache.is_empty() {
            let text = self
                .tokenizer
                .decode(&self.token_cache, self.skip_special_tokens)?;
            let printable_text = text[self.print_len..].to_string();
            self.token_cache.clear();
            self.print_len = 0;
            self.print(printable_text, true)?;
        }
        Ok(())
    }
}

fn is_chinese_char(c: char) -> bool {
    matches!(c,
        '\u{4E00}'..='\u{9FFF}' | '\u{3400}'..='\u{4DBF}'
    )
}
